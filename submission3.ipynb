{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03182206",
   "metadata": {},
   "source": [
    "# CheckThat Task 2\n",
    "\n",
    "The goal of this notebook is to find a method to extract a claim from a passage of text. For example:\n",
    "\n",
    "\n",
    "- **Passage**: Hydrate YOURSELF W After Waking Up Water 30 min Before a Meal DRINK Before Taking a Shower →→ Before Going to Bed at the correct time T A YE Helps activate internal organs Helps \n",
    "\n",
    "    digestion Helps lower blood pressure Helps to avoid heart attack Health+ by Punjab Kesari\n",
    "\n",
    "- **Claim**: Drinking water at specific times can have different health benefits\n",
    "\n",
    "\n",
    "The passage of text will be the input to the method, and the claim will be the output. To evaluate our method, we will use the **METEOR** metric on the **CLEF2025** dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9cf8f6",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "The dataset will be a collection of text passage's and corresponding claims that have been extracted from the dataset. Let's go ahead and download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d93f7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.data_utils import download\n",
    "\n",
    "TRAIN_URL = \"https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task2/data/train/train-eng.csv?inline=false\"\n",
    "TEST_URL = \"https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task2/data/dev/dev-eng.csv?inline=false\"\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "download(TRAIN_URL, \"data\")\n",
    "download(TEST_URL, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c126c1",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Now we make our dataset to hold the downloaded CSV data. This will allow us to iterate through our data easier. Each index of the dataset will return a text claim pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0852075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 11374\n",
      "Test dataset length: 1171\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import ClaimVerificationDataset\n",
    "\n",
    "train_dataset = ClaimVerificationDataset(f\"data/train-eng.csv\")\n",
    "test_dataset = ClaimVerificationDataset(f\"data/dev-eng.csv\")\n",
    "\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Test dataset length: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04312e",
   "metadata": {},
   "source": [
    "## Method 1: Base Model Few-Shot Prompting\n",
    "\n",
    "For this method, we will get a baseline score to see how well a model can do without finetuning. A larger model will be chosen here with the hopes that it will be able to get the best results \n",
    "\n",
    "due to having more parameters. The model will be access via the Together API. We will be using Llama 3.3 with 70 billion parameters. Additionally, our prompting strategy will employ few shot \n",
    "\n",
    "prompting. Input-output pairs will be taken from the dataset and put into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bc87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a diverse set of training samples in the model prompt\n",
    "few_shot_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant designed to extract claims from a given passage of text. Keep it short and return the claim in the text. Only return the big idea and exclude unneeded details.\"},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[0][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[0][\"claim\"]},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[1][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[1][\"claim\"]},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[15][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[15][\"claim\"]},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[23][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[23][\"claim\"]},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[29][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[29][\"claim\"]},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0430f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jwilder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from agents.together_api_agent import TogetherAgent\n",
    "from utils.metrics import evaluate_on_dataset\n",
    "\n",
    "together_agent = TogetherAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c1bcaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LLM claim extraction:  17%|█▋        | 199/1171 [27:16<2:13:13,  8.22s/it]\n"
     ]
    }
   ],
   "source": [
    "data, avg_score = evaluate_on_dataset(test_dataset, together_agent, few_shot_prompt, limit=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628f8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average METEOR score for base LLM few-shot prompting method: 0.291637\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average METEOR score for base LLM few-shot prompting method: {avg_score}\") # 0.2916"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54394d",
   "metadata": {},
   "source": [
    "# Method 2: LLM Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ea50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.metrics import evaluate_claim_extraction\n",
    "\n",
    "model_checkpoint = 'google/flan-t5-large'\n",
    "model_code = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(f\"./{model_code}/finetuned_{model_code}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70152fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_finetuned_model(limit: int = 2000):\n",
    "    data = []\n",
    "    scores = []\n",
    "\n",
    "    counter = 0\n",
    "    for entry in tqdm(test_dataset, desc=\"Evaluating LLM claim extraction\"):\n",
    "        if counter > limit: break\n",
    "        user_prompt = entry[\"text\"]\n",
    "\n",
    "        user_prompt = (\n",
    "        \"Please read the following social media post and extract the claim made within it. \"\n",
    "        \"Normalize the claim by rephrasing it in a clear and concise manner.\\n\\n\"\n",
    "        f\"Post: {user_prompt}\\n\\nExtracted Claim:\"\n",
    "        )\n",
    "\n",
    "        #prompt.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "        #output = agent.ask(prompt)\n",
    "\n",
    "        inputs = tokenizer(user_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=5, early_stopping=True)\n",
    "\n",
    "        output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "        meteor_score = evaluate_claim_extraction(entry[\"claim\"], output)\n",
    "\n",
    "        data.append({\n",
    "            \"ground_truth_claim\": entry[\"claim\"],\n",
    "            \"generated_claim\": output,\n",
    "            \"meteor_score\": meteor_score\n",
    "        })\n",
    "\n",
    "        scores.append(meteor_score)\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0\n",
    "\n",
    "    return data, avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e19f175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LLM claim extraction: 100%|██████████| 1171/1171 [49:24<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "finetuned_data, finetuned_avg_score = evaluate_finetuned_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c312540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average METEOR score for finetuned LLM method: 0.5568712211784799\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average METEOR score for finetuned LLM method: {finetuned_avg_score}\") # 0.5569"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
