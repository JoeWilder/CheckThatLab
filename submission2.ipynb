{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheckThat Task 2\n",
    "\n",
    "The goal here is to find a method to extract a claim from a passage of text. For example:\n",
    "\n",
    "\n",
    "- **Passage**: Hydrate YOURSELF W After Waking Up Water 30 min Before a Meal DRINK Before Taking a Shower ‚Üí‚Üí Before Going to Bed at the correct time T A YE Helps activate internal organs Helps digestion Helps lower blood pressure Helps to avoid heart attack Health+ by Punjab Kesari\n",
    "\n",
    "- **Claim**: Drinking water at specific times can have different health benefits\n",
    "\n",
    "\n",
    "To evaluate our method, we will use the **METEOR** metric on the **CLEF2025** dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "The dataset will be a collection of text passage's and corresponding claims that have been extracted from the dataset. Let's go ahead and download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.data_utils import download\n",
    "\n",
    "TRAIN_URL = \"https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task2/data/train/train-eng.csv?inline=false\"\n",
    "TEST_URL = \"https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task2/data/dev/dev-eng.csv?inline=false\"\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "download(TRAIN_URL, \"data\")\n",
    "download(TEST_URL, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Now we make our dataset to hold the downloaded CSV data. This will allow us to iterate through our data easier. Each index of the dataset will return a text claim pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 11374\n",
      "Test dataset length: 1171\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import ClaimVerificationDataset\n",
    "\n",
    "train_dataset = ClaimVerificationDataset(f\"data/train-eng.csv\")\n",
    "test_dataset = ClaimVerificationDataset(f\"data/dev-eng.csv\")\n",
    "\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Test dataset length: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset successfully exported to data/reduced-train-eng.csv\n",
      "Subset successfully exported to data/reduced-test-eng.csv\n"
     ]
    }
   ],
   "source": [
    "train_dataset.export_subset_to_csv(\"data/reduced-train-eng.csv\")\n",
    "test_dataset.export_subset_to_csv(\"data/reduced-test-eng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Train dataset length: 568\n",
      "Reduced Test dataset length: 58\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import ClaimVerificationDataset\n",
    "\n",
    "reduced_train_dataset = ClaimVerificationDataset(f\"data/reduced-train-eng.csv\")\n",
    "reduced_test_dataset = ClaimVerificationDataset(f\"data/reduced-test-eng.csv\")\n",
    "\n",
    "print(f\"Reduced Train dataset length: {len(reduced_train_dataset)}\")\n",
    "print(f\"Reduced Test dataset length: {len(reduced_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset of Dataset for quick initial testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Base Model Prompt Engineering\n",
    "\n",
    "For this method, we will get a baseline score to see how well a model can do without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the death penalty for so-called \"hate speech\" is not enough. You must scrap the entire Bill, send it back to Singapore where it came from, get on your grubby and scabby knees and ask God and the Nigerian people for forgiveness for having the temerity to try to deprive them of their right to speak freely.\n",
      "\n",
      "(Femi Fani-Kayode) Removing the death penalty for so-called \"hate speech\" is not enough. You must scrap the entire Bill, send it back to Singapore where it came from, get on your grubby and scabby knees and ask God and the Nigerian people for forgiveness for having the temerity to try to deprive them of their right to speak freely.\n",
      "\n",
      "(Femi Fani-Kayode) Removing the death penalty for so-called \"hate speech\" is not enough. You must scrap the entire Bill, send it back to Singapore where it came from, get on your grubby and scabby knees and ask God and the Nigerian people for forgiveness for having the temerity to try to deprive them of their right to speak freely.\n",
      "\n",
      "(Femi Fani-Kayode) None \n",
      " Nigeria's anti-hate speech bill came from Singapore\n"
     ]
    }
   ],
   "source": [
    "index = 80\n",
    "print(train_dataset[index][\"text\"], \"\\n\", train_dataset[index][\"claim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant designed to extract claims from a given passage of text. Keep it short and return the claim in the text. Only return the big idea and exclude unneeded details.\"},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[0][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[0][\"claim\"]},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[1][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[1][\"claim\"]},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[15][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[15][\"claim\"]},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[23][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[23][\"claim\"]},\n",
    "            {\"role\": \"user\", \"content\": train_dataset[29][\"text\"]},\n",
    "            {\"role\": \"assistant\", \"content\": train_dataset[29][\"claim\"]},\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.together_api_agent import TogetherAgent\n",
    "\n",
    "together_agent = TogetherAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Joseph\n",
      "[nltk_data]     Wilder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.metrics import evaluate_on_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, avg_score = evaluate_on_dataset(reduced_test_dataset, together_agent, base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before added in 3 samples: 0.288298275862069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: LLM Finetuning\n",
    "\n",
    "Please view testing.ipynb for the finetuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Wilder\\AppData\\Local\\Temp\\ipykernel_15856\\3436702455.py:2: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.6.0+cu118)\n",
      "    Python  3.10.11 (you have 3.10.0)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "class FinetunedAgent():\n",
    "    def __init__(self):\n",
    "        max_seq_length = 1024\n",
    "        dtype = None\n",
    "        load_in_4bit = True\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"model-good/3B_finetuned_llama3.2\",\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit\n",
    "        )\n",
    "\n",
    "        self.model = FastLanguageModel.for_inference(self.model)\n",
    "\n",
    "    def ask(self, prompt: str):\n",
    "        instruction = \"You are an AI assistant designed to extract claims from a given passage of text. Keep it short and return the claim in the text. Only return the big idea and exclude unneeded details.\"\n",
    "        \n",
    "        class StopOnTokens(StoppingCriteria):\n",
    "            def __init__(self, stop_ids):\n",
    "                self.stop_ids = stop_ids\n",
    "            \n",
    "            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "                for stop_id in self.stop_ids:\n",
    "                    if input_ids[0][-1] == stop_id:\n",
    "                        return True\n",
    "                return False\n",
    "        \n",
    "        if not prompt or len(prompt.strip()) == 0:\n",
    "            return \"No input provided\"\n",
    "        \n",
    "        inference_prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "        ### Instruction:\n",
    "        {instruction}\n",
    "\n",
    "        ### Input:\n",
    "        {prompt}\n",
    "\n",
    "        ### Response:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                [inference_prompt], \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            \n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                min_new_tokens=10,\n",
    "                use_cache=True,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,\n",
    "                stopping_criteria=[\n",
    "                    StoppingCriteriaList([\n",
    "                        StopOnTokens(\n",
    "                            stop_ids=[\n",
    "                                self.tokenizer.eos_token_id,\n",
    "                                self.tokenizer.convert_tokens_to_ids(\"### Input:\"),\n",
    "                                self.tokenizer.convert_tokens_to_ids(\"### Instruction:\")\n",
    "                            ]\n",
    "                        )\n",
    "                    ])\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            full_response = self.tokenizer.batch_decode(outputs)[0]\n",
    "            print(\"Full raw response:\", full_response)\n",
    "            \n",
    "            response_parts = full_response.split(\"### Response:\")\n",
    "            \n",
    "            if len(response_parts) > 1:\n",
    "                answer = response_parts[1].split(\"###\")[0].strip()\n",
    "            else:\n",
    "                answer = full_response.split(\"### Response:\")[-1].strip()\n",
    "            \n",
    "            if not answer:\n",
    "                answer = \"No claim extracted\"\n",
    "            \n",
    "            return answer\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in claim extraction: {e}\")\n",
    "            return \"Error during claim extraction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joseph Wilder\\OneDrive - USNH\\Semester2\\knowledgegraphs\\CheckThatLab\\venv\\lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    NVIDIA GeForce GTX 1660 Ti. Num GPUs = 1. Max memory: 6.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 7.5. CUDA Toolkit: 11.8. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.18 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "finetuned_agent = FinetunedAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.metrics import evaluate_claim_extraction\n",
    "from utils.dataset import ClaimVerificationDataset\n",
    "\n",
    "def evaluate_on_dataset_finetuned(test_dataset: ClaimVerificationDataset, agent: FinetunedAgent, limit = None):\n",
    "    data = []\n",
    "    scores = []\n",
    "\n",
    "    counter = 0\n",
    "    for entry in tqdm(test_dataset, desc=\"Evaluating LLM claim extraction\"):\n",
    "        user_prompt = entry[\"text\"]\n",
    "        output = agent.ask(user_prompt)\n",
    "        meteor_score = evaluate_claim_extraction(entry[\"claim\"], output)\n",
    "\n",
    "        data.append({\n",
    "            \"ground_truth_claim\": entry[\"claim\"],\n",
    "            \"generated_claim\": output,\n",
    "            \"meteor_score\": meteor_score\n",
    "        })\n",
    "\n",
    "        scores.append(meteor_score)\n",
    "\n",
    "        counter += 1\n",
    "        if limit and counter >= limit:\n",
    "            break\n",
    "\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0\n",
    "\n",
    "    return data, avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LLM claim extraction:   2%|‚ñè         | 19/1171 [02:08<2:10:15,  6.78s/it]\n"
     ]
    }
   ],
   "source": [
    "data, avg_score = evaluate_on_dataset_finetuned(test_dataset, finetuned_agent, limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00401\n"
     ]
    }
   ],
   "source": [
    "print(avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to need more training. The responses seem to be struggling with the longer length text. With some shorter prompts I am able to get good results. Best results I have seen\n",
    "with a fine tuned model so far is ~0.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
